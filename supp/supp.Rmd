---
title: "Supplemental Materials for: A robust and readily implementable method for the meta-analysis of response ratios with and without missing standard deviations"
author: Shinichi Nakagawa, Daniel W. A. Noble, Malgorzata Lagisz, Rebecca Spake, Wolfgang Viechtbauer and Alistair M. Senior 
date: "`r Sys.Date()`"
bibliography: ./bib.bib
csl: ./ecology-letters.csl
output: 
  bookdown::html_document2:
    code_folding: hide
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, tidy = TRUE)
options(digits=2)
```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
#install.packages("devtools")
remotes::install_github("rlesur/klippy")
klippy::klippy(tooltip_message = 'Click to Copy Code', tooltip_success = 'Done', position = 'right', color = "red")
```

# **Citation**
Please cite the following paper for the methods proposed for dealing with missing standard deviations: 

Shinichi Nakagawa, Daniel W. A. Noble, Malgorzata Lagisz, Rebecca Spake, Wolfgang Viechtbauer and Alistair M. Senior. 2022. A robust and readily implementable method for the meta-analysis of response ratios with and without missing standard deviations. 

# **General Introduction**

In this supplement we show readers how they can implement some of the missing data approaches covered within our paper to deal with missing standard deviations when using log response ratios (*lnRR*). We provide code snippets that can be copied and added to the users consol. We make use of our worked example from the main manuscript to demonstrate how to apply the methods. We then turn to a second worked example where the authors applied similar methods to deal with missing standard deviations.

Importantly, this tutorial is self-contained. The code will download all the necessary files needed to run the examples directly to your working directory. This includes a subset of data from @bird2019, along with their pruned phylogenetic tree, and data from @mcdonald2019. For simplicity, we have not provided the code here to show readers how to prune the tree and process the data. That code can all be found on our [GitHub](https://github.com/AlistairMcNairSenior/Miss_SD_Sim) repository. Similarly, if users experience any issues with downloading the files needed to run any code they are hosted on [OSF](https://osf.io/h9x6w/). 

# **`cv_avg` Function**: Calculating between study $CV^2$
To facilitate application of the methods we have written a general function for calculating the weighted $CV^2$ across studies. That function is in the `func.R` script, but we provide some detail here. 

```{r echo = TRUE, results='hide', tidy=FALSE, tidy.opts=list(width.cutoff=80), class.source='klippy'}
#' @title cv_avg
#' @description Calculates the weighted average CV^2 within a study and the weighted average CV^2 across a study
#' @param x Mean of an experimental group
#' @param sd Standard deviation of an experimental group
#' @param n The sample size of an experimental group
#' @param group Study, grouping or cluster variable one wishes to calculate the within and between weighted CV^2. In meta-analysis this will most likely be 'study'.
#' @param data The dataframe containing the mean, sd, n and grouping variables
#' @param label A character string specifying the label one wishes to attach to columns to identify the treatment. Otherwise, if not specified it will default to the variable name for x
#' @param sub_b A logical indicating whether the between study CV^2 (b_CV2) should be appended to the data only ('TRUE') or whether both within study CV^2 (w_CV2), mean sample size (n_mean) and between study CV^2 (b_CV2) should all be appended to the data only ('FALSE')

cv_avg <- function(x, sd, n, group, data, label = NULL, sub_b = TRUE){

  # Check if the name is specified or not. If not, then assign it the name of the mean, x, variable input in the function. https://stackoverflow.com/questions/60644445/converting-tidyeval-arguments-to-string
  if(is.null(label)){
    label <- purrr::map_chr(enquos(x), rlang::as_label)
  }

  # Calculate between study CV. Take weighted mean CV within study, and then take a weighted mean across studies of the within study CV. Weighted based on sample size and pooled sample size.
  b_grp_cv_data <- data                                             %>%
    dplyr::group_by({{group}})                            %>%
    dplyr::mutate(w_CV2 = weighted.mean(na_if(({{x}} / {{sd}})^2, Inf), 
                                        {{n}},na.rm = TRUE),
                  n_mean = mean({{n}}, na.rm = TRUE))   %>%
    dplyr::ungroup(.)                                     %>%
    dplyr::mutate(b_CV2 = weighted.mean(w_CV2, n_mean, na.rm = TRUE), .keep = "used")

  # Make sure that label of the calculated columns is distinct from any other columns
  names(b_grp_cv_data) <- paste0(names(b_grp_cv_data), "_", label)

  # Append these calculated columns back to the original data and return the full dataset.
  if(sub_b){
    b_grp_cv_data <- b_grp_cv_data %>% dplyr::select(grep("b_", names(b_grp_cv_data)))
    dat_new <- cbind(data, b_grp_cv_data)
  } else {
    dat_new <- cbind(data, b_grp_cv_data)
  }

  return(data.frame(dat_new))
}

```

The function will take the mean (`x`), standard deviation (`sd`) and sample size (`n`) along with the desired dataframe and calculate the between study $CV^2$ that is needed for many of the methods implemented. The `group` argument is needed to identify the "study" / cluster variable which is important so that the function first takes the weighted mean $CV^2$ within the study first before calculating the $CV^2$ across studies. The new $CV^2$ is then added directly to the dataframe. You can label these new columns using the `label` argument, otherwise, it will append the name of the column for the treatment group with `b_CV2`. We'll show how this function is used as we overview the methods in detail below.

# **Worked Example 1**: Dealing with missing SD data to explore the fitness impacts of competition between herbivorous insects

### Introduction
@bird2019 conducted a meta-analysis exploring the impacts of competition on herbivorous insect fitness when occupying the same host plant with another species or in isolation. Building on work by  @Kaplan2007, they collected data on a series of fitness measurements [e.g., abundance, body size, development time, fecundity; see Table 1 in @bird2019] and explored the overall impacts of competition on the various fitness measures independently and in composite analyses. @bird2019 also tested the importance of a series of moderators they predicted would impact the magnitude of competition between species including population density, phylogenetic distance, diet breadth and spatial and temporal separation. A phylogeny was constructed using DNA sequence data and this gene tree was used to control for phylogenetic non-independence within analyses. 

For demonstration purposes, we focus on a subset of fitness data, abundance, and use a simple multilevel meta-analytic model to estimate the overall impact of competition on focal insect fitness (i.e., intercept or overall meta-analytic mean) while controlling for phylogeny, research group, and research year [as per the analysis by @bird2019]. Our use of log response ratio (lnRR) meant that we could only use a subset of abundance data from @bird2019 because of lnRR requiring ratio scale data. In addition, the ratio of minimum to largest sampling error variance calculated from the raw data was high suggesting some errors in the original published papers.  To avoid model convergence issues we excluded these data and used a sample comprised of 293 effect sizes across 67 unique focal insect species with known phylogenetic relationships. We then introduced missing data at the paper level so that ~20% of papers had effect sizes with missing SD in the control and experimental treatment; a scenario that is typical of many meta-analyses. 

### Setting up

First, load the libraries and source functions:

```{r echo = TRUE, eval = TRUE, class.source='klippy', results='hide'}
#install.packages("pacman")
pacman::p_load(tidyverse, metafor, here, osfr, ape, phytools)

# Useful functions for calculating CV^2 within and between studies
osfr::osf_retrieve_file("https://osf.io/sqr4w/") %>% osfr::osf_download(conflicts = "overwrite")
source("./func.R")
```

Then, download the data file and phylogenetic tree file that we will use to demonstrate the examples

```{r echo = TRUE, eval = TRUE, results='hide', class.source='klippy'}
# Download the data file from OSF
osfr::osf_retrieve_file("https://osf.io/evysw/") %>% osfr::osf_download(conflicts = "overwrite") 

# Download the phylogeny from OSF
osfr::osf_retrieve_file("https://osf.io/t5kh4/") %>% osfr::osf_download(conflicts = "overwrite") 

# Load the data file and tree file
data1 <- read.csv(here::here("example1.csv"))
 tree <- read.tree(here::here("phylo_tree"))
```

The data from @bird2019 is incomplete. It is missing many SD values in the control and treatment group, which would normally mean we would have to exclude these data prior to analysis (Figure \@ref(fig:fig1))

```{r fig1, fig.cap = "Missing data in the Bird et al. (2019) dataset", echo=FALSE, eval = TRUE, include=TRUE, message=FALSE, warning=FALSE}
na.plot <- data1 %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato3'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number") +
    coord_flip()

na.plot
```

### Method 1A:  

In our manuscript we provide the whole data and complete case analysis. Here, we jump straight into showing readers how to implement the different 'solutions' to the missing standard deviation problem that is typical when conducting meta-analysis.

```{r cvavg, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE, class.source='klippy'}
# Calculate the average between study CV, which will replace missing values.
    data1 <- cv_avg(x = Control_mean, sd = Control_standard_deviation,
                            n = Control_sample_size, group = Author, label = "1",
                             data = data1)
    data1 <- cv_avg(x = Experimental_mean, sd = Experimental_standard_deviation,
                            n = Experimental_sample_size, group = Author,
                            label = "2", data = data1)
```

# **Worked Example 2**: Dealing with missing SD data when assessing strategic-rest grazing (SRG) regimes on both ungrazed and constantly grazed (CG) systems

### Introduction
Our second example is from @mcdonald2019 which demonstrates nicely a real dataset with  missing standard deviation data as a result of it not being reported within papers. @mcdonald2019 studied the effects of strategic-rest grazing (SRG) regimes on both ungrazed and constantly grazed (CG) systems. They looked at a number of different ecological outcomes. Here, we focus on their data on the effects of SRG vs CG on biomass; this dataset contains 173 effect sizes from 67 studies. In their original analysis @mcdonald2019 find that the biomass of CG systems is significantly reduced relative to that SRG, but do not report the total heterogeneity. The dataset contains two dimensions of non-independence that are common to eco-evolutionary meta-analyses; 1) multiple effect sizes per study and 2) several effect sizes within the same study are computed as relative to the same control group [sometimes termed ‘stochastic dependency’; @Noble2017; @GleserOlkin2009].

### Setting up


```{r echo = TRUE, eval = TRUE, results='hide', class.source='klippy'}
# Download the data file from OSF
osfr::osf_retrieve_file("https://osf.io/at6pn/") %>% osfr::osf_download(conflicts = "overwrite") 

# Load the data file and tree file
data2 <- read.csv(here::here("example2.csv"))
```

Of the 173 effect sizes in the biomass dataset, 35.8% have missing SD data (Figure \@ref(fig:fig2)). Where missing, SDs were missing for both the CG (`CSD` in data) and SRG (`TSD` in data) treatment groups in the effect size (Figure \@ref(fig:fig2)). In their original analyses @mcdonald2019 handle these missing data by calculating the average CV from all studies without missing data. They then use the reported mean value for studies with missing SDs coupled with the average CV to impute the missing SD value. This method is similar to single imputation of missing SDs, by predicting their value from a mean-SD linear regression. Non-independence was handled by @mcdonald2019 using MLMA, which included a variance-covariance matrix to account for stochastic dependency.

```{r fig2, fig.cap = "Missing data in the McDonald et al. (2019) dataset.", echo=FALSE, eval = TRUE, include=TRUE, message=FALSE, warning=FALSE}
na.plot2 <- data2 %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato3'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number") +
    coord_flip()

na.plot2
```

In table 1 we present the results of re-analysis of the biomass data from @mcdonald2019, again using MLMA, but with the four different methods to handles missing SDs. For reference we also include the results of a complete cases analysis where studies with missing SDs have been excluded. The effect sizes for the different methods are all very similar, although the CI for the complete cases analysis is wider than for those that include studies with missing SDs. Method 1B estimated slightly lower heterogeneity than the other methods.

# **Session Information**
```{r echo = FALSE, eval = TRUE}
sessionInfo() %>% pander::pander()
```

# **References**
